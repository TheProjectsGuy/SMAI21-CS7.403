{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0_wToNFHMN3"
      },
      "source": [
        "# Decision Trees\n",
        "\n",
        "Decision trees using scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Wisconsin Breast Cancer Dataset(WBCD) can be found here(https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)\n",
        "\n",
        "This dataset describes the characteristics of the cell nuclei of various patients with and without breast cancer. The task is to classify a decision tree to predict if a patient has a benign or a malignant tumour based on these features.\n",
        "\n",
        "Attribute Information:\n",
        "```\n",
        "#  Attribute                     Domain\n",
        "   -- -----------------------------------------\n",
        "   1. Sample code number            id number\n",
        "   2. Clump Thickness               1 - 10\n",
        "   3. Uniformity of Cell Size       1 - 10\n",
        "   4. Uniformity of Cell Shape      1 - 10\n",
        "   5. Marginal Adhesion             1 - 10\n",
        "   6. Single Epithelial Cell Size   1 - 10\n",
        "   7. Bare Nuclei                   1 - 10\n",
        "   8. Bland Chromatin               1 - 10\n",
        "   9. Normal Nucleoli               1 - 10\n",
        "  10. Mitoses                       1 - 10\n",
        "  11. Class:                        (2 for benign, 4 for malignant)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following powershell commands were run to download the dataset\n",
        "\n",
        "```pwsh\n",
        "Invoke-WebRequest -Uri https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data -OutFile breast-cancer-wisconsin.data\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import everything\n",
        "import numpy as np\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qYdlWpUVHMOB"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CT</th>\n",
              "      <th>UCSize</th>\n",
              "      <th>UCShape</th>\n",
              "      <th>MA</th>\n",
              "      <th>SECSize</th>\n",
              "      <th>BN</th>\n",
              "      <th>BC</th>\n",
              "      <th>NN</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.417740</td>\n",
              "      <td>3.134478</td>\n",
              "      <td>3.207439</td>\n",
              "      <td>2.806867</td>\n",
              "      <td>3.216023</td>\n",
              "      <td>3.463519</td>\n",
              "      <td>3.437768</td>\n",
              "      <td>2.866953</td>\n",
              "      <td>1.589413</td>\n",
              "      <td>2.689557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.815741</td>\n",
              "      <td>3.051459</td>\n",
              "      <td>2.971913</td>\n",
              "      <td>2.855379</td>\n",
              "      <td>2.214300</td>\n",
              "      <td>3.640708</td>\n",
              "      <td>2.438364</td>\n",
              "      <td>3.053634</td>\n",
              "      <td>1.715078</td>\n",
              "      <td>0.951273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               CT      UCSize     UCShape          MA     SECSize          BN  \\\n",
              "count  699.000000  699.000000  699.000000  699.000000  699.000000  699.000000   \n",
              "mean     4.417740    3.134478    3.207439    2.806867    3.216023    3.463519   \n",
              "std      2.815741    3.051459    2.971913    2.855379    2.214300    3.640708   \n",
              "min      1.000000    1.000000    1.000000    1.000000    1.000000    0.000000   \n",
              "25%      2.000000    1.000000    1.000000    1.000000    2.000000    1.000000   \n",
              "50%      4.000000    1.000000    1.000000    1.000000    2.000000    1.000000   \n",
              "75%      6.000000    5.000000    5.000000    4.000000    4.000000    5.000000   \n",
              "max     10.000000   10.000000   10.000000   10.000000   10.000000   10.000000   \n",
              "\n",
              "               BC          NN     Mitoses   Diagnosis  \n",
              "count  699.000000  699.000000  699.000000  699.000000  \n",
              "mean     3.437768    2.866953    1.589413    2.689557  \n",
              "std      2.438364    3.053634    1.715078    0.951273  \n",
              "min      1.000000    1.000000    1.000000    2.000000  \n",
              "25%      2.000000    1.000000    1.000000    2.000000  \n",
              "50%      3.000000    1.000000    1.000000    2.000000  \n",
              "75%      5.000000    4.000000    1.000000    4.000000  \n",
              "max     10.000000   10.000000   10.000000    4.000000  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualize data\n",
        "headers = [\"ID\",\"CT\",\"UCSize\",\"UCShape\",\"MA\",\"SECSize\",\"BN\",\"BC\",\"NN\",\n",
        "    \"Mitoses\",\"Diagnosis\"]\n",
        "data = pd.read_csv('breast-cancer-wisconsin.data', na_values='?',    \n",
        "         header=None, index_col=['ID'], names = headers) \n",
        "data = data.reset_index(drop=True)\n",
        "data = data.fillna(0)\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_gQq5qrHMOG"
      },
      "source": [
        "## Part 1: Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Implement a Decision Tree\n",
        "\n",
        "1. a) Implement a decision tree (you can use decision tree implementation from existing libraries)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g6R3GmzBHMOH"
      },
      "outputs": [],
      "source": [
        "# Data for the classifier\n",
        "np_data = np.array(data)\n",
        "# Training and testing split\n",
        "x_train, x_test, y_train, y_test = train_test_split(np_data[:,:-1], \n",
        "    np_data[:,-1], test_size=30, shuffle=True)\n",
        "# Classifier using sklearn\n",
        "clf = tree.DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: [2. 2. 2. 2. 4. 2. 2. 2. 4. 2. 4. 2. 2. 2. 2. 2. 2. 4. 2. 2. 4. 2. 2. 2.\n",
            " 2. 2. 2. 4. 2. 4.]\n",
            "Actual: [2. 2. 4. 2. 4. 2. 2. 2. 4. 2. 4. 2. 2. 2. 2. 2. 2. 4. 2. 2. 4. 2. 2. 2.\n",
            " 2. 2. 2. 4. 2. 4.]\n",
            "Errors: 1\n"
          ]
        }
      ],
      "source": [
        "# Train it\n",
        "clf.fit(x_train, y_train)\n",
        "# Test it\n",
        "y_pred = clf.predict(x_test)\n",
        "print(f\"Predictions: {y_pred}\")\n",
        "print(f\"Actual: {y_test}\")\n",
        "print(f\"Errors: {np.sum(y_pred != y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save as a DOT file\n",
        "dot_data = tree.export_graphviz(clf, out_file=\"tree1.dot\", \n",
        "    feature_names=headers[1:-1], class_names=[\"Benign\", \"Malignant\"],\n",
        "    filled=True, rounded=True, special_characters=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The decision tree was converted from DOT to PNG using [onlineconvertfree](https://onlineconvertfree.com/convert-format/dot-to-png/). The tree is shown below (for a particular run)\n",
        "\n",
        "<img alt=\"First decision tree\" src=\"./figures/tree1.png\" width=1000 />\n",
        "\n",
        "This could be done using graphviz, but the current build seems to have an [issue](https://stackoverflow.com/questions/69989691/how-to-resolve-attributeerror-module-graphviz-backend-has-no-attribute-encod) on Windows platforms (as of 24 Nov 2021 at 11:30 AM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.97303922, 0.02696078]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predict a probability value (based on entropy values of node)\n",
        "clf.predict_proba(x_test[[2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ7N9m_mHMOJ"
      },
      "source": [
        "### Task 2: Experiment\n",
        "\n",
        "1. b) Train a decision tree object of the above class on the WBC dataset using misclassification rate, entropy and Gini as the splitting metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eHFij6PaHMOJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(criterion='entropy')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Different metrics\n",
        "clf_gini = tree.DecisionTreeClassifier(criterion='gini')\n",
        "clf_ent = tree.DecisionTreeClassifier(criterion='entropy')\n",
        "# Training\n",
        "clf_gini.fit(x_train, y_train)\n",
        "clf_ent.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXEjInvmHMOK"
      },
      "source": [
        "### Task 3: Accuracies\n",
        "\n",
        "1. c) Report the accuracies in each of the above splitting metrics and give the best result.\n",
        "\n",
        "Accuracy here is determined based on number of misclassifications (mismatches) on the test set. More the misclassifications, lesser will be the accuracy (as it'll generalize worse). Accuracy is therefore the percentage of test samples classified correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "49QZvmgNHMOL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gini: 1 wrongs, 96.67% accurate\n",
            "\tDepth = 10\n",
            "\tNumber of leaves = 37\n",
            "Entropy: 2 wrongs, 93.33% accurate\n",
            "\tDepth = 9\n",
            "\tNumber of leaves = 31\n"
          ]
        }
      ],
      "source": [
        "# Accuracies\n",
        "mis_gini = np.sum(y_test != clf_gini.predict(x_test))\n",
        "mis_ent = np.sum(y_test != clf_ent.predict(x_test))\n",
        "acc_gini = 1 - (mis_gini / len(y_test))\n",
        "acc_ent = 1 - (mis_ent / len(y_test))\n",
        "print(f\"Gini: {mis_gini} wrongs, {acc_gini*100:.2f}% accurate\")\n",
        "print(f\"\\tDepth = {clf_gini.get_depth()}\")\n",
        "print(f\"\\tNumber of leaves = {clf_gini.get_n_leaves()}\")\n",
        "print(f\"Entropy: {mis_ent} wrongs, {acc_ent*100:.2f}% accurate\")\n",
        "print(f\"\\tDepth = {clf_ent.get_depth()}\")\n",
        "print(f\"\\tNumber of leaves = {clf_ent.get_n_leaves()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gini was more accurate (with more leaves), but both were nearly the same. Different runs could also give the entropy method a higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Result\n",
        "p1t3_best_clt = clf_gini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz_7nYxPHMON"
      },
      "source": [
        "### Task 4: Experiment more\n",
        "\n",
        "1. d) Experiment with different approaches to decide when to terminate the tree (number of layers, purity measure, etc). Report and give explanations for all approaches. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yLRI0niJHMOP"
      },
      "outputs": [],
      "source": [
        "# List of all hyperparameters to test\n",
        "test_hps = [    # max_depth, criterion, splitter\n",
        "    [8, 'gini', 'random'],      # 0\n",
        "    [5, 'gini', 'best'],        # 1\n",
        "    [5, 'gini', 'random'],      # 2\n",
        "    [1, 'gini', 'random'],      # 3\n",
        "    [8, 'entropy', 'random'],   # 4\n",
        "    [5, 'entropy', 'best'],     # 5\n",
        "    [5, 'entropy', 'random'],   # 6\n",
        "    [3, 'entropy', 'random'],   # 7\n",
        "    [1, 'entropy', 'best'],     # 8\n",
        "]\n",
        "clfs = []   # Store all classifiers\n",
        "acc_vals = []   # Accuracy values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 0 acc: 93.33%\n",
            "Test 1 acc: 96.67%\n",
            "Test 2 acc: 96.67%\n",
            "Test 3 acc: 86.67%\n",
            "Test 4 acc: 100.00%\n",
            "Test 5 acc: 96.67%\n",
            "Test 6 acc: 96.67%\n",
            "Test 7 acc: 96.67%\n",
            "Test 8 acc: 90.00%\n"
          ]
        }
      ],
      "source": [
        "# Generate all classifiers\n",
        "for i, (max_d, c, s) in enumerate(test_hps):\n",
        "    # Classifier\n",
        "    clf = tree.DecisionTreeClassifier(criterion=c, splitter=s, \n",
        "        max_depth=max_d)\n",
        "    # Train\n",
        "    clf.fit(x_train, y_train)\n",
        "    # Evaluate\n",
        "    num_misc = np.sum(y_test != clf.predict(x_test))\n",
        "    acc = 1 - (num_misc / len(y_test))\n",
        "    print(f\"Test {i} acc: {acc*100:.2f}%\")\n",
        "    # Store\n",
        "    clfs.append(clf)\n",
        "    acc_vals.append(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Various hyper-parameters were tested in the `test_hps` list. Ultimately, the hyper-parameters that increased accuracy were kept, and those that reduced accuracy were removed. A fair mix of the gini and entropy classification criteria was done, along with even the splitting criteria. Different depths of the tree was also explored (along with the interesting case of _single_ depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the tree for preview\n",
        "tree.export_graphviz(clfs[4], out_file=\"tree2.dot\", \n",
        "    feature_names=headers[1:-1], class_names=[\"Benign\", \"Malignant\"],\n",
        "    filled=True, rounded=True, special_characters=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some interesting _single_ decision trees were also found. Note that they actually satisfy (most of) the test dataset. An example is shown below\n",
        "\n",
        "<img src=\"./figures/single-dtree.png\" width=300 alt=\"Single decision on UCSize\" />\n",
        "\n",
        "This is no better than a hard threshold on a single feature. However, a tree with more decisions (nodes) will probably generalize better since it takes more features into account. An example is the tree below\n",
        "\n",
        "<img src=\"./figures/fivel-dtree.png\" width=1000 alt=\"Decision tree with more nodes\" />\n",
        "\n",
        "It can be seen that through either case (in the top decision), there are chances of getting `Malignant` or `Benign` based on various factors. \n",
        "\n",
        "Note that some of the leaf nodes in the image above have entropy values that are high. You can consider them to be more-or-less inconclusive (less sure) decisions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWAN_wWXHMOQ"
      },
      "source": [
        "## Part 2: Random Forests\n",
        "\n",
        "2. What is boosting, bagging and  stacking?\n",
        "Which class does random forests belong to and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnO5uqHlHMOR"
      },
      "source": [
        "### Section 1: Theory\n",
        "\n",
        "Description of the terms and the reason why random forests should be bagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Ensemble Learning**\n",
        "\n",
        "Ensemble learning is a method of using multiple weak learners to create a strong learner. Combining the predictions from multiple learners (models) yields a model that can form a stronger consensus on the prediction (majority vote, for example). Compared to single models, this type of learning can yield improved efficiency and accuracy. The two main types of ensemble learning techniques are _bagging_ and _boosting_. In bagging, weak learners are trained in parallel whereas in boosting, weak learners are trained sequentially. Usually bagging is used on weak learners that have high variance and low bias (overfitting) whereas boosting is used when there is high bias and low variance (underfitting), but this isn't a fixed rule.\n",
        "\n",
        "**Boosting**\n",
        "\n",
        "Boosting is a set of ensemble techniques that require training in a sequential manner. Some types are briefly described below\n",
        "-  Adaptive boosting is a method of identifying misclassified data-points and adjusting their weights so that in the next iteration, they're learned correctly. This way, the algorithm gradually improves over time.\n",
        "- Gradient boosting is a method that adds predictors to an ensemble with each one correcting for the errors of its predecessor. It trains on the residual errors of the previous predictor.\n",
        "- Extreme gradient boosting (XGBoost) allows gradient boosting to happen efficiently, using parallelization.\n",
        "\n",
        "**Bagging**\n",
        "\n",
        "Bagging involves forming multiple models and having them reach a consensus. The algorithm (as introduced by [Leo Breiman](https://doi.org/10.1023/A:1018054314350)) involves the following steps\n",
        "1. Bootstrapping: Creating a diverse selection of the dataset (with replacement, that is a sample can occur across various selections). These samples (smaller datasets) are called bootstrap samples.\n",
        "2. Parallel training: These samples are the trained on models independently of one-another. The result is that various weak learners can be produced, who potentially learn different / diverse aspects of the data.\n",
        "3. Aggregation: An average or a majority vote of these learners is taken to decide the output.\n",
        "\n",
        "> Reference: IBM Blog on [bagging](https://www.ibm.com/cloud/learn/bagging) and [boosting](https://www.ibm.com/cloud/learn/boosting)\n",
        "\n",
        "**Stacking**\n",
        "\n",
        "Stacking, or stacked generalization, is an ensemble algorithm that involves combining multiple machine learning models in an efficient manner. There are base models that are individual models that train on a dataset, and then there are meta-models that learn how to generate the correct predictions given the predictions of each of these models. The goal is for the meta-model to learn the trusting weightage of the base models by understanding the areas in which the particular base models differ in their predictions.\n",
        "\n",
        "> Reference: Blog [machinelearningmastery](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/)\n",
        "\n",
        "**Random Forests**\n",
        "\n",
        "Random forests is basically an algorithm that creates a voting on decision trees. So it is more like a bagging algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihvGbqLHMOS"
      },
      "source": [
        "### Section 2: Implementing Random Forests\n",
        "\n",
        "3. Implement random forest algorithm using different decision trees.\n",
        "\n",
        "> **Note**: Do not use inbuilt libraries for implementing random forests. The least expected parameters that are to be given as input are Max_depth, min_size, n_trees, n_features, criterion, and of course the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXdPP2aIHMOT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJOn5nNZHMOU"
      },
      "source": [
        "### Section 3: Comparison\n",
        "\n",
        "4. Report the accuracies obtained after using the Random forest algorithm and compare it with the best accuracies obtained with the decision trees. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce4KiiIGHMOV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj-vNvsYHMOX"
      },
      "source": [
        "## Part 3: Separate solution\n",
        "\n",
        "5. Submit your solution as a separate pdf in the final zip file of your submission\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute a decision tree with the goal to predict the food review based on its smell, taste and portion size.\n",
        "\n",
        "<img src=\"./figures/decision_trees.png\" alt=\"Dataset for food reviews\" width=400 />\n",
        "\n",
        "(a) Compute the entropy of each rule in the first stage.\n",
        "\n",
        "(b) Show the final decision tree. Clearly draw it.\n",
        "\n",
        "Submit a handwritten response. Clearly show all the steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Decision_trees.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1f8d80d535cfd832283e4e3a1095d2ce45fe6627336684f2622a1965babb2f1c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit (windows store)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
